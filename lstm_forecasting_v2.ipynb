{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPATs1H6X+BopV8DjNp1D/1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ofakduman/Lstm-forecasting/blob/main/lstm_forecasting_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOrppxDnXThw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"all_data.csv\")"
      ],
      "metadata": {
        "id": "VMRCeG_OYIK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### logcs nan olanlari at, record ID tekrarlanmasin\n",
        "### Food name kolonunu kaldir\n",
        "### Temperature Nan lari kaldir ve temprature kolonundaki logcs degerleri gibi olan degerleri kaldir deger int e cevriliyorsa kalsin\n",
        "### Nanlari kaldir Aw deki\n",
        "### Assumed kolonunu kaldir\n",
        "### Conditions kolonunu kaldir\n",
        "### Max.rate(logc.conc / h) kolonunu kaldir"
      ],
      "metadata": {
        "id": "hVEWZXTnZQqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Organism'].unique()\n",
        "df['Organism'].isna().sum()\n"
      ],
      "metadata": {
        "id": "aL7jfF0OZ3Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df['Food category'].unique())\n",
        "df['Food category'].isna().sum()\n"
      ],
      "metadata": {
        "id": "ORwQdFbIbs-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df['Food Name'].unique())\n",
        "# df['Food Name'].isna().sum()"
      ],
      "metadata": {
        "id": "GezEgqSOb5oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Temperature (C)'].unique()\n",
        "# df['Temperature (C)'].isna().sum()"
      ],
      "metadata": {
        "id": "7bRABaEWhr2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Aw'].unique()\n",
        "df['Aw'].isna().sum()"
      ],
      "metadata": {
        "id": "NFLrNUojli4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pH'].unique()\n",
        "df['pH'].isna().sum()"
      ],
      "metadata": {
        "id": "VJc3Yi-JZbvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Assumed'].unique()\n",
        "df['Assumed'].isna().sum()\n",
        "# df.columns\n"
      ],
      "metadata": {
        "id": "tm-FxfXXZqM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Max.rate(logc.conc / h)'].unique())\n",
        "# df['Max.rate(logc.conc / h)'].isna().sum()\n"
      ],
      "metadata": {
        "id": "ffpOqJRYaCKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a_TsiCTiaUl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace(\"Not available\", pd.NA, inplace=True)\n",
        "len(df)\n"
      ],
      "metadata": {
        "id": "3Mlx1vipYPjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped = df.drop(['Conditions', 'Assumed', 'Max.rate(logc.conc / h)', 'Food Name'], axis=1)\n",
        "df_dropped.head()\n",
        "len(df_dropped.dropna())"
      ],
      "metadata": {
        "id": "KAlVig_ObY8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "df.groupby('Organism').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "6xSjszmNYz1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "aSUUTBewYpqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_dropped)\n",
        "df_dropped\n"
      ],
      "metadata": {
        "id": "Us7PxDgRaS5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = df_dropped.drop_duplicates(subset='Record ID', keep='first')"
      ],
      "metadata": {
        "id": "U6lNY4VodF20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_logcs(logcs_str):\n",
        "    # Değerleri ; ile böl ve float'a çevir\n",
        "    values = list(map(float, logcs_str.split(';')))\n",
        "\n",
        "    # Zaman ve populasyon değerlerini ayır\n",
        "    times = values[::2]\n",
        "    populations = values[1::2]\n",
        "\n",
        "    # Populasyon değerlerinin artıp artmadığını kontrol et\n",
        "    increases = 0\n",
        "    decreases = 0\n",
        "    for i in range(1, len(populations)):\n",
        "        if populations[i] > populations[i-1]:\n",
        "            increases += 1\n",
        "        elif populations[i] < populations[i-1]:\n",
        "            decreases += 1\n",
        "\n",
        "    # Toplam değer sayısı ve artma/azalma sayılarına göre bir sonuç döndür\n",
        "    return {\n",
        "        \"total_values\": len(populations),\n",
        "        \"increases\": increases,\n",
        "        \"decreases\": decreases\n",
        "    }\n",
        "\n",
        "# `Logcs` kolonunu analiz edelim\n",
        "cleaned_df_copy = cleaned_df.copy()\n",
        "results = cleaned_df['Logcs'].apply(analyze_logcs)\n",
        "\n",
        "# Sonuçları yeni kolonlarda saklayalım\n",
        "cleaned_df_copy['Total Values'] = results.apply(lambda x: x[\"total_values\"])\n",
        "cleaned_df_copy['Increases'] = results.apply(lambda x: x[\"increases\"])\n",
        "cleaned_df_copy['Decreases'] = results.apply(lambda x: x[\"decreases\"])"
      ],
      "metadata": {
        "id": "mfSjI3OIiwo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df_copy[cleaned_df_copy[\"Record ID\"] == \"Lm_BfG_31\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "vn3Kk1doi_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = \"0;8.7;0.12;7.88;0.17;6.15;0.25;5.78;0.33;4.97;0.42;3.87;0.5;2.34;0.75;1.69;0.83;1.39\"\n",
        "\n",
        "a.split(\";\")"
      ],
      "metadata": {
        "id": "u3B9G_xrxbXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Örnek Series verisi\n",
        "data = {\"Logcs\": \"0;8.7;0.12;7.88;0.17;6.15;0.25;5.78;0.33;4.97;0.42;3.87;0.5;2.34;0.75;1.69;0.83;1.39\"}\n",
        "series = pd.Series(data)\n",
        "\n",
        "# String'i noktalı virgül ile ayır\n",
        "values = series['Logcs'].split(';')\n",
        "\n",
        "# Değerlerin sayısını bul\n",
        "number_of_points = len(values)\n",
        "\n",
        "print(\"Zaman serisindeki nokta sayısı:\", number_of_points)\n"
      ],
      "metadata": {
        "id": "qK8guPuWykdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "# Veri setinizi yükleyin\n",
        "# cleaned_df_copy = pd.read_csv('your_dataset.csv')  # Örnek veri seti yolu\n",
        "# Sayısal sütunları seçin (örnek olarak sadece sayısal verileri içeren sütunlar listelenmiştir)\n",
        "numeric_columns = ['Temperature (C)', 'Aw', 'pH', 'Total Values', 'Increases', 'Decreases']\n",
        "for col in numeric_columns:\n",
        "    # Sayısal olmayan verileri ve NaN değerleri temizle\n",
        "    cleaned_data = cleaned_df_copy[col].apply(pd.to_numeric, errors='coerce').dropna()\n",
        "    squared_data = cleaned_data ** 2\n",
        "\n",
        "    # Histogramı çiz\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(cleaned_data, kde=True)\n",
        "    plt.title(f'Histogram of {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "    # Temel İstatistiksel Özellikler\n",
        "    print(f\"\\n{col} - Statistical Species:\")\n",
        "    print(f\"Min: {cleaned_data.min()}\")\n",
        "    print(f\"Max: {cleaned_data.max()}\")\n",
        "    print(f\"Mean: {cleaned_data.mean()}\")\n",
        "    print(f\"Median: {cleaned_data.median()}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VszhhJ0hjtFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df_copy[cleaned_df_copy['Record ID'] == 'PS203']"
      ],
      "metadata": {
        "id": "SDX2_Tzdj3P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trashold_sample_value = 5\n",
        "df = cleaned_df_copy[cleaned_df_copy['Total Values'] > trashold_sample_value]\n",
        "df = df[df['Temperature (C)'].apply(lambda x: ';' not in x)]\n",
        "df = df[df['pH'].apply(lambda x: ';' not in x)]\n",
        "df = df[df['Aw'].apply(lambda x: ';' not in x)]\n",
        "len(df)"
      ],
      "metadata": {
        "id": "kvkzspI4kNnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_logcs(logcs_str):\n",
        "    pairs = logcs_str.split(';')\n",
        "    parsed_pairs = []\n",
        "    for i in range(0, len(pairs) - 1, 2):\n",
        "        try:\n",
        "            # Zaman değerini tam sayıya dönüştür\n",
        "            time = float(pairs[i])\n",
        "            # Değer olarak ondalık sayı kullan\n",
        "            value = float(pairs[i + 1])\n",
        "            parsed_pairs.append((time, value))\n",
        "        except ValueError:\n",
        "            # Geçersiz dönüşüm olduğunda bu çifti atla\n",
        "            continue\n",
        "    return parsed_pairs\n",
        "\n",
        "df['Logcs'] = df['Logcs'].apply(parse_logcs)\n"
      ],
      "metadata": {
        "id": "5sxmdb8MkOqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n"
      ],
      "metadata": {
        "id": "45egECMdkTXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_columns = set(df.columns)\n",
        "\n",
        "df = pd.get_dummies(df, columns=['Organism', 'Food category'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KhD81ViXthsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Pandas'ın maksimum gösterim sınırlarını ayarlayın\n",
        "pd.set_option('display.max_rows', 100)  # Satırları sınırsız göster\n",
        "pd.set_option('display.max_columns', 100)  # Sütunları sınırsız göster\n",
        "\n",
        "# DataFrame'i göster\n",
        "df\n"
      ],
      "metadata": {
        "id": "xdLvaN1I0d7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "features_to_scale = ['Temperature (C)', 'Aw', 'pH']\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Normalizasyonu sadece belirli sütunlara uygula\n",
        "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n"
      ],
      "metadata": {
        "id": "DGkkkbQovps1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"Record ID\"]==\"Lm_BfG_31_1\"]"
      ],
      "metadata": {
        "id": "7H25Kcadt5-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Hariç tutulacak sütunlar\n",
        "exclude_columns = ['Record ID','Total Values', 'Increases', 'Decreases','Logcs']\n",
        "\n",
        "# all_features listesini oluşturma\n",
        "all_features = [col for col in df.columns if col not in exclude_columns]\n",
        "\n",
        "# Veriyi hazırlama\n",
        "X = []  # Girdiler\n",
        "y = []  # Hedefler\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    logcs_data = row['Logcs']\n",
        "    # Tüm uygun özellikleri al\n",
        "    features = row[all_features].values\n",
        "    for i in range(len(logcs_data)-1):\n",
        "        X.append(np.concatenate(([logcs_data[i][1]], features)))  # current value and other features\n",
        "        y.append(logcs_data[i+1][1])  # next value\n",
        "\n",
        "X = np.array(X).reshape(-1, 1, len(X[0]))  # (samples, time steps, features)\n",
        "y = np.array(y).reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "tkPOyIR8tym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Verilerinizi yükleyin veya oluşturun\n",
        "# X ve y verilerini bu aşamada hazırlayın\n",
        "\n",
        "# Verileri eğitim ve test kümesine bölün\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "X_train = X_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)\n",
        "\n",
        "# LSTM modeli oluşturun\n",
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "model.add(layers.LSTM(32, return_sequences=True))\n",
        "model.add(layers.Dense(1))  # Çıkış katmanı\n",
        "\n",
        "# Modeli derleyin\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')  # İstediğiniz bir optimizasyon ve loss fonksiyonu kullanabilirsiniz\n",
        "\n",
        "# Early Stopping callback'i oluşturun\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# Modeli eğitin ve Early Stopping callback'ini kullanın\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Modelin performansını değerlendirin\n",
        "loss = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "\n",
        "# Tahminler yapın\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Eğitim ve doğrulama kayıplarını görselleştirin\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3FQbFqQI2dBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Tahminleri ve gerçek değerleri alın\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Linear Regression modelini oluşturun ve eğitin\n",
        "regression_model = LinearRegression()\n",
        "y_test = y_test.reshape(-1, 1)  # Yeniden şekillendirme\n",
        "predictions = predictions.reshape(-1, 1)  # Yeniden şekillendirme\n",
        "regression_model.fit(y_test, predictions)\n",
        "\n",
        "# Gerçek değerlerin minimum ve maksimum değerlerini alın\n",
        "min_val = min(np.min(y_test), np.min(predictions))\n",
        "max_val = max(np.max(y_test), np.max(predictions))\n",
        "\n",
        "# Doğruyu çizmek için tahminler oluşturun\n",
        "x_values = np.linspace(min_val, max_val, 100)\n",
        "y_values = regression_model.predict(x_values.reshape(-1, 1))\n",
        "\n",
        "# Scatter plot oluşturun\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, predictions, alpha=0.5)\n",
        "plt.plot(x_values, y_values, color='red', linewidth=2, label='Regression Line')\n",
        "\n",
        "# X=Y doğrusunu çizmek için\n",
        "plt.plot([min_val, max_val], [min_val, max_val], color='blue', linewidth=2, label='X=Y Line')\n",
        "\n",
        "plt.xlabel('Values')\n",
        "plt.ylabel('Predictions')\n",
        "plt.title('Values vs. Predictions')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m1NqMJii21pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import numpy as np\n",
        "# Tahminlerin boyutunu kontrol edin ve düzeltin\n",
        "predictions = np.squeeze(predictions)\n",
        "\n",
        "# Ortalama Mutlak Hata (MAE)\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.3f}\")\n",
        "\n",
        "# Kök Ortalama Karesel Hata (RMSE)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.3f}\")\n",
        "\n",
        "# R² Skoru\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(f\"R² Score: {r2:.3f}\")\n"
      ],
      "metadata": {
        "id": "RMcYJ-D03Dll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)\n",
        "df.head(1)"
      ],
      "metadata": {
        "id": "JVaLOKJd4tc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXEebs9l7i10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# df DataFrame'ini ilk olarak eğitim ve test setlerine ayır\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# df_train DataFrame'ini daha sonra eğitim ve doğrulama setlerine ayır\n",
        "df_train_partial, df_val = train_test_split(df_train, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "ERKhGXik8WWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(1)"
      ],
      "metadata": {
        "id": "q0uY5H3-88vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare_data(df):\n",
        "  X, y = [], []\n",
        "  exclude_columns = ['Record ID','Total Values', 'Increases', 'Decreases','Logcs']\n",
        "\n",
        "  # all_features listesini oluşturma\n",
        "  all_features = [col for col in df.columns if col not in exclude_columns]\n",
        "  all_features\n",
        "\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "      logcs_data = row['Logcs']\n",
        "      # all_features kullanarak özellikleri seç\n",
        "      features = row[all_features].values\n",
        "\n",
        "      if len(logcs_data) > 5:\n",
        "          X_input = []\n",
        "          for i in range(5):\n",
        "              X_input.extend([logcs_data[i][1]])\n",
        "              X_input.extend(features)\n",
        "\n",
        "          y_output = [logcs_data[i][1] for i in range(5, len(logcs_data))]\n",
        "\n",
        "          X.append(X_input)\n",
        "          y.append(y_output)\n",
        "\n",
        "  return np.array(X).reshape(-1, 5, len(X[0]) // 5), np.array(y, dtype=object)\n",
        "\n",
        "# Modelin geri kalan kısmı aynı kalabilir\n",
        "X_train, y_train = prepare_data(df_train_partial)\n",
        "X_val, y_val = prepare_data(df_val)\n",
        "X_test, y_test = prepare_data(df_test)\n"
      ],
      "metadata": {
        "id": "Nfx_9F3l7tXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
        "\n",
        "# Maksimum çıktı uzunluğunu hesapla\n",
        "max_output_length = max(len(y) for y in y_train)\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(5, len(X_train[0][0])))  # 5 zaman adımı, özellik sayısı\n",
        "encoder_lstm = LSTM(50, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = RepeatVector(max_output_length)(encoder_outputs)  # Burada encoder çıktısını tekrarla\n",
        "decoder_lstm = LSTM(50, return_sequences=True)\n",
        "decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = TimeDistributed(Dense(1))  # Her zaman adımı için bir tahmin\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Model\n",
        "model = Model(encoder_inputs, decoder_outputs)\n",
        "\n",
        "# Model Derleme\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Model Özeti\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "U0vN3lHQ9NGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "epochs = 10  # Toplam epoch sayısı\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    total_train_loss = 0\n",
        "    total_val_loss = 0\n",
        "\n",
        "    # Eğitim döngüsü\n",
        "    for i in range(len(X_train)):\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_train[i:i+1], training=True)\n",
        "            train_loss = loss_fn(y_train[i], y_pred[0, :len(y_train[i]), 0])\n",
        "        gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        total_train_loss += train_loss.numpy()\n",
        "\n",
        "    # Doğrulama döngüsü\n",
        "    for i in range(len(X_val)):\n",
        "        # X_val[i:i+1] dizisini tensöre dönüştür\n",
        "        X_val_tensor = tf.convert_to_tensor(X_val[i:i+1], dtype=tf.float32)\n",
        "        y_pred_val = model.predict(X_val_tensor)\n",
        "\n",
        "        # y_val[i] dizisini tensöre dönüştür\n",
        "        y_val_tensor = tf.convert_to_tensor(y_val[i], dtype=tf.float32)\n",
        "\n",
        "        # Tahmin ve gerçek değerlerin boyutlarını eşitle\n",
        "        min_length = min(y_pred_val.shape[1], len(y_val[i]))\n",
        "        val_loss = loss_fn(y_val_tensor[:min_length], y_pred_val[0, :min_length, 0])\n",
        "\n",
        "        total_val_loss += val_loss.numpy()\n",
        "\n",
        "    # Ortalama kayıpları hesapla ve kaydet\n",
        "    avg_train_loss = total_train_loss / len(X_train)\n",
        "    avg_val_loss = total_val_loss / len(X_val)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    print(f\"Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "# Kayıpları grafik üzerinde görselleştirme\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.title('Train & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gjnPRey39TaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test veri seti üzerinde tahmin yapma\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "5rf9DuGp9wYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "def calculate_metrics(y_true, y_pred):\n",
        "    mse_values = []\n",
        "    mae_values = []\n",
        "    r2_values = []\n",
        "\n",
        "    for i in range(len(y_true)):\n",
        "        true_values = np.array(y_true[i])\n",
        "        pred_values = y_pred[i, :len(y_true[i]), 0]\n",
        "\n",
        "        mse = mean_squared_error(true_values, pred_values)\n",
        "        mae = mean_absolute_error(true_values, pred_values)\n",
        "\n",
        "        mse_values.append(mse)\n",
        "        mae_values.append(mae)\n",
        "\n",
        "        # R2 skorunu yalnızca birden fazla değeri olan örnekler için hesap\n",
        "        if len(true_values) > 1:\n",
        "            r2 = r2_score(true_values, pred_values)\n",
        "            r2_values.append(r2)\n",
        "\n",
        "    return np.mean(mse_values), np.mean(mae_values), np.mean(r2_values) if r2_values else np.nan\n",
        "\n",
        "# Hata metriklerini hesaplama\n",
        "mse, mae, r2 = calculate_metrics(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "print(\"R2 Score:\", r2)\n"
      ],
      "metadata": {
        "id": "xYhCNTOhM4tT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "selected_indices = random.sample(range(len(y_test)), 5)\n",
        "\n",
        "for index in selected_indices:\n",
        "    plt.figure(figsize=(8, 4))  # Grafik boyutu\n",
        "    plt.plot(y_test[index], marker='o', label='y_test')\n",
        "\n",
        "    # y_pred verisini y_test verisinin uzunluğu\n",
        "    y_pred_trimmed = y_pred[index][:len(y_test[index])]\n",
        "    plt.plot(y_pred_trimmed, marker='x', label='y_pred')\n",
        "\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.title(f'Sample {index + 1}')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "o0GTZwpIO2pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Krs5VXobO5gZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}